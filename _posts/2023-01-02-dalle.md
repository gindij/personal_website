---
title: How does OpenAI's DALL-E work?
date: 2023-01-02
tags: [computer-science, machine-learning]     # TAG names should always be lowercase
use_math: true
---

# Introduction
Before I say or you read anything else, create an account and try [this](https://openai.com/dall-e-2/) out.
DALL-E is one of the very exciting examples of **generative AI**, in which the algorithms essentially look
at a large amount of training data and learn how to generate new examples. The most exciting, viral
examples of generative AI that have emerged over the past year have been in the fields of computer vision 
([Stable Diffusion](https://stablediffusionweb.com), DALL-E) and natural language processing ([Chat GPT](https://chat.openai.com)),
and in this post, I want to try to provide a very high-level overview of how DALL-E learns to generate images.

## What we will not discuss (but should at least mention)
The "machine learning" that powers many of these large, impressive models is very often not the hardest part of
developing them. There are computational considerations and optimizations that are critically
important to making such models, but I don't think they will add a lot of value for my readers, so we won't be
going into those details. (In fact, in the DALL-E paper, they explicitly acknowledge that the hardest part
was "[g]etting the model to train in 16-bit precision past one billion paremeters, without diverging.")

## Disclaimer
This post is my high-level overview/summary of part of the [DALL-E paper](https://arxiv.org/pdf/2102.12092.pdf). It is possible
that I misunderstood something or explained it incorrectly. If you come across any such mistakes, let me know (1) so that I
can learn from them and (2) so that I can correct them.

With that out of the way, let's have a look under the hood.

# High level overview
DALL-E learns using about 250 million (image, text) pairs. The learning process is roughly:
1. Learn how to come up with useful (and compressed) representations of images
2. Turn each prompt into a sequence of tokens
3. Combine the representations of the images and the corresponding prompts and train
   a neural network to predict the next token of the combined representation given previous 
   tokens.

We will discuss each of these steps in turn.

# Learning image representations
Neural networks like numbers (not images or text). In order to bring the full power of
neural networks to bear on text and vision problems, we usually use approaches that
are variations on this theme:
1. Convert the image into a numerical representation such that the meaning/content of the image
   is preserved. (For example, in representation space, collections of numbers corresponding images 
   of cats should be closer to one another than they are to collections of numbers corresponding to 
   images of galaxies.) We will refer to these as representations or embeddings.
2. Use the representations for some task that we care about (e.g. classifying an image as cat or not).

There are many approaches that accomplish step 1. In the case of DALL-E, OpenAI used an 
**autoencoder**. This architecture consists of two parts:
1. Encoder: takes an image and produces a representation
2. Decoder: takes the representation and tries to reproduce the image

In order to learn, the encoder and decoder use penalties incurred for differences between the original image and the
decoder's reconstruction to update their internal states. When training is complete, we can use 
the encoder to produce useful image representations for whatever other tasks we intend to carry out with the
images as input. Intuitively, we can think about an autoencoder as a kind of compression algorithm. We take an image, compress it
into a representation that is (1) smaller than the original image, but (2) such that we can (mostly) reconstruct 
the original image from it. If we can do this well, our compressed representation seems to carry much of the 
important information from the original image, which is what we wanted.

The autoencoder that DALL-E uses compresses 256x256 (dimensions in pixels) images into 32x32 representations. Each 256x256
image has 3 numbers associated with each pixel (red, green, and blue concentrations). Thus, each original image
requires 256 * 256 * 3 = 196,608 numbers to represent it, whereas the representation only requires 32 * 32 = 1024 numbers.
This is a compression factor of 192! 

(Note: Representations of this kind are often continuous, meaning the numbers in each representation slot can be
any real number. In this case, the encoding is discrete, which just means that the numbers in each representation slot
are actually whole numbers instead.)

# Encoding prompts
To encode the prompts, DALL-E uses byte-pair encoding, which can be more broadly be categorized as a **tokenization** method.
Tokenization is a way of breaking down unstructured natural language into a finite set of meaningful atomic units. It can be carried 
out at the level of sentences, words, or even parts of words (e.g., "eating" might be broken into "eat" and "ing"). Each token is usually assigned a 
positive whole number (there is usually one such number reserved for unknown tokens), and we can then use our tokenization mapping
(of words to numbers) to convert a corpus of text into an ordered sequence of tokens. 

We can manually decide the level at which to tokenize text (e.g., words, sentences) or we can use machine learning to learn
a good tokenization, but we aren't going to go into detail about that here.

# Putting it together
Once we have computed our image representations and tokenized our text, we glue the representations together into what is 
essentially a representation for an (text, image) pair. We then train a model called a transformer whose inputs are the entire 
stream of concatenated text and image tokens. The model learns to predict the next token in the sequence from the previous ones. 

Transformers are based on a mechanism called **(self-)attention**. In our case, this means that 
the model learns to give different weight to different previously generated tokens as it attempts to predict
the next one. For example, if we were trying to predict the next word ("cold") in the sentence "I need a jacket because it is ___",
the model would learn that it should give the word "jacket" more weight than the word "I." 

(DALL-E actually uses multi-head attention, which means it actually learns many weighting schemes at the same time for a given
set of previously generated tokens and then makes a prediction using a combination of the outputs of all of those schemes.)

# Using the model
Once we've trained the transformer from the previous section, given a new prompt that we haven't seen, we can
encode it and then generate (hopefully reasonable) image tokens one-by-one. The first image
token would be generated using all of the text tokens, the second image token would use all
text tokens and the first image token, and so on. (Note: This is not exactly how it works, but it's close enough for our
purposes.) Finally, we can use the decoder that we trained when we were learning the image representations to decode the 
generated image tokens back into an image.

# Conclusion
I glossed over many of the mathematical and computational details of how this works (I don't even have my head around all of them!), 
but the goal was to demystify one approach used to build a(n awesome) generative AI system. Hope it was useful!