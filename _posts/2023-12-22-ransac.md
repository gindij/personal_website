---
title: RANSAC for robust data fitting
date: 2023-12-22
tags: [machine-learning, computer-science]     # TAG names should always be lowercase
use_math: true
---

## Introduction
In this post I want to introduce a non-standard way of fitting a mathematical model to data
that I came across during a course I took this past semester in computer vision. While gradient
descent rules the day (as it should!) the method we discuss here is actually pretty clever and
its simplicity belies a significant benefit: robustness to outliers.

## Linear models
### What are they?
Much of what we try to do in machine learning and statistical modeling can be roughly
characterized with the following sequence of steps:
1. Gather data about some phenomenon or process, often (input, output) pairs
2. Fit a useful mathematical model to the data, i.e., learn some function $\hat f$
   that maps the inputs to the outputs without too much error
3. Use the function $\hat f$ to make predictions on new, unseen examples

(I call the function $\hat f$ here because one way of thinking about what we're
doing is that we're trying to approximate some true function $f$ that relates the
inputs to the outputs.)

There is a lot of nuance and subtlety to how we learn $\hat f$, how we verify that it's
working well on new examples, but that's the basic idea.

One choice we the modelers have to make before learning $\hat f$ is its "shape," which
encodes an assumption about the underlying input/output relationship. The simplest
possible model we can use is called a linear model, which assumes that the output
changes linearly (usually with some small amount of random variation) as the input changes.

### How do we learn them?
To make things more mathematically precise, let's say our inputs $x_i$ are $d$-dimensional
vectors of real numbers ($x_i \in \mathbf{R}^d$), and our corresponding outputs
$y_i$ are real numbers ($y_i \in \mathbf{R}$). Using a linear model to model the
relationship is equivalent to making the assumption that there is a vector of
parameters $\theta \in \mathbf{R}^d$ (the slope) and an intercept $b \in \mathbf{R}$
such that
<div>
\begin{align*}
    y_i = \theta^\top x_i + b + \varepsilon_i
\end{align*}
</div>
where $\varepsilon_i$ is some randomness that our model doesn't capture.

The mathematical characterization of the problem of finding the optimal parameters $\theta^\star$
and $b^\star$ is
<div>
\begin{align*}
    \theta^\star, b^\star = \text{arg min}_{\theta, b} \| \theta^\top X - b\mathbf{1} - y \|_2^2
\end{align*}
</div>

To actually solve that problem, we typically use one of two methods:
1. Solve some equations using calculus and linear algebra (because in this case we can)
2. Use an algorithm called gradient descent (for cases when we can't). This algorithm
   is the workhorse of almost all modern machine learning.

Neither of those is the topic of this post, but there are lots of nice articles
out there if you are interested in learning more. For the rest of _this_ post, I want
to describe another less widely-known algorithm for finding those paramters.

## RANSAC
One deficiency of the approaches mentioned above is that without certain auxiliary techniques,
both of those methods are sensitive to outliers, as shown in the simple figure below ([source](https://tillbe.github.io/outlier-influence-identification.html)):
![image info](outlier.png)

Our intuition would suggest that a line that more closely fits points 1-7 would be a better model for whatever
process was generating this data.

One simple and interesting way of finding $\theta$
that is robust to outliers is called **RA**ndom **SA**mpling **C**onsensus (RANSAC).
The way it works is as follows:
1. Randomly select a subset of $d$ input/output $(x, y)$ pairs. Stack the $x_i$s into a matrix $\tilde X$
   ($x_i$ is the $i$th column) and the $y_i$s into a vector $\tilde y$.
2. Solve $\theta^\top \tilde X = \tilde y$ for $\theta$ (provided certain conditions are met, the equation
   has a unique solution).
3. Across the entire original dataset $X$, count inliers, or the number of $(x, y)$ pairs such that
   $\text{dist}(\theta^\top x, y)$ is small (the modeler chooses a threshold).

In order to increase our chances of discovering a favorable parameter combination, we can repeat this
process until the inlier count (perhaps as a fraction of the overall dataset size) is sufficiently high.

The primary disadvantage of RANSAC is that there is no guarantee that the algorithm will converge, which
is a fancy way of saying that we can't guarantee that it will zero in on a good set of paramters. The tradeoff
we can make is that the more times we repeat steps 1-3, the better a chance we have of finding a good model.

## Application to (old-school) computer vision
I came across RANSAC in a computer vision course at NYU taught by Robert Fergus. Near the end of the course,
after reveling in the various and wondrous ways that neural networks have upended and redefined how computers process
and, more recently, create, visual information, we had a final unit on techniques in computer vision that
predated the deep learning revolution.

One problem that one might want to solve in computer vision is to find some kind of correspondence between
two images. For example, given the two images of the same scene on the left, we might want to produce
a mosaic image like the one on the right ([source](https://cs.nyu.edu/~fergus/teaching/vision/12_descriptors_matching.pdf)):
![image info](mosaic.png)

Let's say that we waved a magic wand and had (1) identified "key points" in both of the individual images, and (2) determined
a set of correspondences between the key points in the top and bottom images. What we now need to do is figure out the transformation
that "happened" that caused the points in one image to turn into their (hopefully correct) corresponding points in the other.
It turns out that this transformation has parameters in and we can actually find the correspondence
using RANSAC as follows:
1. Randomly select a subset of matching points from both images. (The number of matching points is determined by the
   number of parameters to estimate, in this case, 6.)
2. Find the parameters of a transformation $T$ that would turn points in one image into their matching points in the other.
3. For each matching pair of points, denoted $(k_1, k_2)$, count the number of inliers, i.e., pairs for with $T(k_1)$
   isn't far from $k_2$.

After finding the transformation that "explains" the largest number of matches, we can proceed to cobble the images
together.

## Conclusion
In this post, we learned about RANSAC, an algorithm that finds the parameters of a model that can handle the presence
of otherwise corruptive outliers. Algorithms that operate by trying a bunch of (literally) random options are generally
the stuff of novices, so it's cool when that very simplicity turns out to solve an important technical problem.
