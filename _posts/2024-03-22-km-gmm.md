---
title: The connection between k-means and Gaussian mixtures
date: 2024-03-22
tags: [machine-learning, statistics]
use_math: true
---


# Introduction
In the next few posts I want to take you on the learning journey that happened when I tried to wrap my head around [this wonderful paper](https://icml.cc/2012/papers/291.pdf) by Brian Kulis and Michael I. Jordan (yes, Michael Jordan). The paper is about a variant of the k-means algorithm inspired by a Bayesian way of thinking about clustering. This helps solve a significant problem with k-means, namely that we don't know the optimal value of $k$ in advance, and, furthermore, that there often is not even a way to make a reasonable guess of what a good value might be for a particular dataset.

In this post, we will go over the contents of section 2.1 of the paper, which is about a connection between k-means and a mixture of Gaussians model. In future posts, we will introduce a Bayesian approach to clustering, and see how that perspective is more than just philosophical; it can help us develop a new algorithm that is more flexible than k-means.

If you're not familiar with k-means, I'd recommend reading up on it before proceeding. The [Wikipedia page](https://en.wikipedia.org/wiki/K-means_clustering) is a good place to start.

# Background
We are going to develop the k-means algorithm in a somewhat non-standard way, but first we need to review a few preliminaries.

## Mixtures of Gaussians
A mixture of Gaussians model can be succinctly expressed as follows:
<div>
\begin{align*}
p(x) = \sum_{i=1}^k \pi_i N(x | \mu_i, \Sigma_i).
\end{align*}
</div>
This expression says that under our model, the probability assigned to sampling a point near $x$ is a mixture of the probabilities of sampling a point near $x$ according to each of $k$ different Gaussians. The mixing coefficient $\pi_i$ represents the amount of weight we place on the $i$th Gaussian (they are nonnegative and sum to 1), and $\mu_i$ and $\Sigma_i$ are the mean( vector)s and covariance( matrice)s, respectively.

One way to interpret this model that I find intuitively appealing is as an assumption that each point in your dataset is generated from one of $k$ distinct (Gaussian) generating processes. To sample a new point from this distribution, you can first sample a value $i$ using the $\pi_i$, and then subsequently sample a point from the corresponding Gaussian. (Note that this assumption may or may not be suitable for a particular dataset or application. As George Box has famously said: "All models are wrong, but some are useful.")

## EM algorithm
The EM algorithm is a way of carrying out [maximum likelihood estimation](https://en.wikipedia.org/wiki/Maximum_likelihood_estimation) in cases where the model is constructed with **latent variables**. Latent variables are often used to account for or represent characteristics of a population that cannot be directly observed. As an example, someone's political orientation might not be directly observable, but if you ask questions about certain issues, you might be able to infer it.

The algorithm tries to find an optimal pair of quantities: the values of the latent variables and the parameter values. To do this, we alternate between two steps

### Expectation step
The output of this step is a *function* that takes a potential parameter setting (i.e., a model) $\theta$, and outputs a score. (Higher scores are better.) The obvious question: How do we compute that score? Let's say that $z$ is a possible latent variable setting. The score corresponding to $z$ is $\log p(x, z | \theta)$, i.e., the (log) likelihood of observing the data $x$ and $z$ assuming the model $\theta$. Since a "probable" setting of $z$ depends on the parameters we've estimated so far and the observed data $x$, the overall score for $\theta$ takes a weighted average of the scores across possible values of $z$, where the weights are the likelihood of observing $z$ given the latest parameters and the data $x$.

To summarize using notation we'll refer to in the next section, the output of this step is a function we'll call $Q$. To formalize the idea in the previous paragraph, we define $Q$ as follows:
<div>
\begin{align*}
Q(\theta; x, \theta^{(t)}) = E_{z \sim p(\cdot | x, \theta^{(t)})} [\log p(x, z | \theta)].
\end{align*}
</div>

(Here, $\theta^{(t)}$ is the latest parameter setting we've estimated. The $t$ is a subscript, not an exponent.)

The semicolon in the definition of $Q$ is just a way of indicating that the function also depends on the data $x$ and the latest parameter setting $\theta^{(t)}$, but that we're treating these as known (whereas $\theta$ is a variable).

### Maximization step
Once we have the function $Q$, we simply maximize the function over $\theta$. The concrete update rule depends on the model we're working with, but the general idea is to find the parameter setting that makes the data most likely. In general mathematical terms we are looking for
<div>
\begin{align*}
\theta^{(t+1)} = \arg\max_{\theta} Q(\theta; x, \theta^{(t)}).
\end{align*}
</div>

We repeat alternate between the expectation and maximization steps until the algorithm converges.

# k-means via a mixture of Gaussians
For each point $x_i$, we can imagine that there is a latent cluster assignment vector $z_i \in \mathbb{R}^k$. Each entry of $z_i$ corresponds to the probability we assign that the point $x_i$ belongs to the corresponding cluster. This is what we call a **soft assignment**. (In contrast, k-means makes a **hard assignment**, where each point is assigned to *exactly one* cluster. We'll get to that in a moment.)

Let's assume that the $x_i$ are drawn from a mixture of Gaussians with mixture coefficients $\pi_i$ for $i = 1,\dots, k$, but with the additional assumption that for each $i$, $\Sigma_i = \sigma I$. That is, we assume that the covariance matrices for each Gaussian are all diagonal and have the same value $\sigma$ on the diagonal.

The entries of the $z_i$ can be expressed as
<div>
\begin{align*}
z_{ij} &= \frac{\pi_j N(x_i | \mu_j, \sigma I)}{\sum_{l=1}^k \pi_l N(x_i | \mu_l, \sigma I)} \\
&= \frac{\pi_j \exp\left(-\frac{1}{2\sigma} \|x_i - \mu_j\|^2\right)}{\sum_{l=1}^k \pi_l \exp\left(-\frac{1}{2\sigma} \|x_i - \mu_l\|^2\right)}.
\end{align*}
</div>
The denominator here is just to make sure that the entries of $z_i$ sum to 1. The numerator is roughly the probability that $x_i$ was generated by the $j$th Gaussian. (The exact expression is a bit more complicated, but this is the intuition.)

The k-means algorithm can be seen as a limiting case of the EM algorithm applied to this mixture of Gaussians setup. The E-step computes $z_i$ as above. Since the covariances are fixed, the M-step only updates the $\mu_j$ to be the weighted average of all of the $x_i$, where each point's contribution is weighted by the corresponding $z_{ij}$. This makes intuitive sense; the farther away a point is from a given mean, the (exponentially) less impact it has on that mean's update.

The final ingredient we need to make the rigorous connection to k-means is to note what happens when $\sigma \to 0$. As we assume that the $k$ Gaussians in our mixture become narrower and narrower, points that are not very close to the means have their likelihoods decay to 0.
In fact, the sum in the denominator of the expression for $z_{ij}$ becomes dominated by the term with the smallest distance to $\mu_j$. This means that as $\sigma \to 0$, $z_{ij}$ tends to 1 when $j$ minimizes $||x_i - \mu_j||^2$ and 0 otherwise. This "one-hot"ing of the $z_i$ is precisely computing a hard assignment, which is what k-means does.

To summarize with a true mouthful: The k-means algorithm can be seen as the application of the EM algorithm to a mixture of Gaussians when we assume that the covariance matrices are fixed to $\sigma I$ and we let $\sigma$ go to 0.

# Conclusion
That's all for now. I've always find it quite satisfying to see how different, seemingly disparate theorems, algorithms, models, etc. can come together in unexpected ways.

In the next post, we'll start to develop our Bayesian perspective to develop the new clustering algorithm I mentioned in the intro. Stay tuned!
